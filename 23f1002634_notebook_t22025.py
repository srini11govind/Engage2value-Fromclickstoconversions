# -*- coding: utf-8 -*-
"""23f1002634-notebook-t22025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/srini11govind/23f1002634-notebook-t22025.42598331-e4a2-45ca-b3a3-5877dd132bab.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20251212/auto/storage/goog4_request%26X-Goog-Date%3D20251212T070009Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D24b263831208af43432de10375cc323799fed4b5aa22a018a9e633a60673b578d3e12a64efc4015143e1fb91c83844ae5b03d3bfa732e396221b9d7aa7d42c85623b4ca70a052ff3de7ccdfff1725f1a54c005782ddcc9bfa3e6bb989cc5fa415b8d501922e22986b9451183b9e87650944c7b5cd6ad4c3de9043791ff381e9b329883f24423e5ed725a2c999c2e3e3bd214bbb12960429bd0980f270b24c13a09546d40f04fa82371a9de2747a3acdf84ae533527ed5bdb0f03abd0b3204711366d5b2550a21249b54ccb78e33dd97b522dbcb916aa598bd383dbcef77abda8dfd69fcfceef764985c33b9815b4fafbf3c3ac2b86ad03df498e71153e977dd3
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

engage_2_value_from_clicks_to_conversions_path = kagglehub.competition_download('engage-2-value-from-clicks-to-conversions')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""**LOAD THE DATA**"""

df1= pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')
df=df1.copy()

"""**IMPORT REQUIRED LIBRARIES MODULES AND FUNCTIONS**"""

from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV,cross_val_score, KFold

from sklearn.preprocessing import StandardScaler,OneHotEncoder,FunctionTransformer
from sklearn.impute import SimpleImputer

from sklearn.base import BaseEstimator,TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.linear_model import LinearRegression, SGDRegressor,Ridge
from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor,VotingRegressor
from sklearn.metrics import r2_score

from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats import loguniform,randint

"""# EDA"""

df.head()

df.tail()

"""** Many values are Nan,not available in demo dataset**
further EDA required
"""

df.shape

"""**116023 rows and 52 columns**
complex data set. some columns to be dropped if possible
"""

df.describe()

"""**target value has many 0s **
Max value is an outlier
"""

df.describe(percentiles=[0.3,0.4],include='number')

df.describe(include='object')

df['purchaseValue'].argmax()

df['purchaseValue'].iloc[42667]

df['purchaseValue'].sort_values(ascending=False).head(50)

df['purchaseValue'].nunique()

df['purchaseValue'].value_counts().tail(6000)

df['purchaseValue'].dtypes

df['purchaseValue'].shape

df['purchaseValue'].kurtosis()

rows=df.loc[df['purchaseValue']==23129500000.0]
print(rows)

df.info()

"""# separate categorical and numeric columns"""

numerical_features= df.select_dtypes(include = ['number']).columns.tolist()
categ_features = df.select_dtypes(include = ['object','category']).columns.tolist()
bool_features = df.select_dtypes(include = ['bool']).columns.tolist()
print (numerical_features)

print (categ_features)

print (bool_features)

"""**Check for how many unique values in each feature**"""

for col in df.columns:
    print(f"{col}:{df[col].nunique()}")

"""** Features with one unique values shall be dropped after checking for not boolean type**"""

df['purchaseValue'].value_counts().head(50)

df['purchaseValue'].nunique()

"""# valuecounts for categoric columns"""

for col in categ_features:
    print(f"{col}:{df[col].value_counts()}")

df['purchaseValue'].sort_values(ascending=False).head(10)

"""# value counts for numewric columns"""

df['purchaseValue'].value_counts().head(50)

for col in numerical_features:
    print(f"{col}:{df[col].value_counts()}")

"""# Identified the boolean features from above

checking for how many notset values in each categoric columns
"""

df[df['trafficSource.campaign'] == '(not set)'].shape[0]

df[df['geoNetwork.region'] == '(not set)'].shape[0]

df[df['trafficSource'] == '(not set)'].shape[0]

df[df['os'] == '(not set)'].shape[0]

df[df['geoNetwork.subContinent'] == '(not set)'].shape[0]

df[df['geoNetwork.city'] == '(not set)'].shape[0]

df['trafficSource.referralPath'].isna().sum()

df['purchaseValue'].value_counts()

"""(out of 116023 rows, 92038 rows are 0 for purchasevalue) 79.3%

seperated bool, numeric and categoric as per above results
"""

num_features=['sessionNumber','pageViews','totalHits','userId','sessionId','trafficSource.adwordsClickInfo.page','sessionStart']

bool_features_F=['trafficSource.isTrueDirect','device.isMobile']
bool_features_T=['trafficSource.adwordsClickInfo.isVideoAd']
bool_features_zero=['totals.bounces','new_visits','gclIdPresent','totals.visits']




cat_features = ['geoNetwork.subContinent','geoNetwork.metro','geoCluster','geoNetwork.networkDomain','trafficSource.medium','deviceType','trafficSource.adwordsClickInfo.slot','trafficSource.adwordsClickInfo.adNetworkType','browser','os','userChannel','geoNetwork.continent','trafficSource.keyword','trafficSource.adContent','trafficSource.campaign','locationCountry','trafficSource','geoNetwork.region','geoNetwork.city','trafficSource.referralPath','device.screenResolution','screenSize','device.mobileDeviceBranding','device.mobileInputSelector','device.mobileDeviceMarketingName','device.operatingSystemVersion','device.flashVersion','geoNetwork.networkLocation','browserMajor','device.browserSize','socialEngagementType','locationZone','device.mobileDeviceModel','device.language','device.browserVersion','device.screenColors']

"""# CORELATION MATRIX"""

df2=df[['sessionNumber','pageViews','totalHits','userId','sessionId','trafficSource.adwordsClickInfo.page','sessionStart','purchaseValue']]
plt.figure(figsize=(10,10))
corr_mat = df2.corr()
sns.heatmap(corr_mat,annot =True,cmap="coolwarm")
plt.title("corr matrix")
plt.show()

"""session number, pageviews and total hits are highly corelated with target. Important numeric features

**Further EDA to drop columns and to group the top n values and remaining as others for features with many unique values**
"""

df.groupby('geoNetwork.continent')['purchaseValue'].mean().sort_values(ascending = False).head(20)

df['locationCountry'].value_counts().head(30)

for x in cat_features:
    print(df.groupby(x)['purchaseValue'].mean())

df['geoNetwork.region'].nunique()

"""**Function to drop columns in a pipeline**"""

class DropColumn(BaseEstimator,TransformerMixin):

    def __init__(self,columns):
        self.columns=columns

    def fit(self,X,y=None):
        return self

    def transform(self,X):
        X1=X
        if isinstance(X1,pd.Series):
            X1=X1.to_frame()
        if not isinstance (X1,pd.DataFrame):
            return X
        return X1.drop(columns=self.columns,errors='ignore')

"""**Filling of blanks in boolean features and converting bool to int**"""

class boolfillint_F(BaseEstimator,TransformerMixin):
    def __init__(self,bool_cols):
        self.bool_cols=bool_cols

    def fit(self,X,y=None):
        return self

    def transform(self,X):
        Z=X.copy()
        for col in self.bool_cols:
            Z[col]=Z[col].fillna(False).astype(bool).astype(int)
        return Z

class boolfillint_T(BaseEstimator,TransformerMixin):
    def __init__(self,bool_cols):
        self.bool_cols=bool_cols

    def fit(self,X,y=None):
        return self

    def transform(self,X):
        Z=X.copy()
        for col in self.bool_cols:
            Z[col]=Z[col].fillna(True).astype(bool).astype(int)
        return Z

class boolfillint_zero(BaseEstimator,TransformerMixin):
    def __init__(self,bool_cols):
        self.bool_cols=bool_cols

    def fit(self,X,y=None):
        return self

    def transform(self,X):
        Z=X.copy()
        for col in self.bool_cols:
            Z[col]=Z[col].fillna(0).astype(int)
        return Z

class boolfillint_one(BaseEstimator,TransformerMixin):
    def __init__(self,bool_cols):
        self.bool_cols=bool_cols

    def fit(self,X,y=None):
        return self

    def transform(self,X):
        Z=X.copy()
        for col in self.bool_cols:
            Z[col]=Z[col].fillna(1).astype(int)
        return Z

"""GROUPING with top n and assigning  LEAST OCCURING VALUES IN A COLUMN  as OTHERS"""

class GroupOthers(BaseEstimator,TransformerMixin):
    def __init__(self,cat_features=None,n=12):
        self.cat_features = [cat_features] if isinstance(cat_features,str) else cat_features
        if self.cat_features is None:
            self.cat_features = []
        self.n =n
        self.top_categories={}

    def fit (self,X,y=None):
        if len(self.cat_features) != 1:
            return self
        col_name = self.cat_features[0]
        X1=X
        if isinstance(X,np.ndarray):
            if X.ndim ==1:
                X1=pd.DataFrame({col_name:X})
            elif X.ndim ==2 and X.shape[1] ==1:
                X1=pd.DataFrame(X,columns=[col_name])
            else:
                return self
        elif isinstance(X,pd.Series):
            X1= X.to_frame(name=col_name)
        elif isinstance (X,pd.DataFrame):
            if col_name not in X.columns or len(X.columns)!= 1:
                return self
            X1= X.copy()
        else:
            return self
        if not isinstance(X1,pd.DataFrame) or col_name not in X1 or len(X1.columns) != 1:
            return self
        if X1[col_name].dtype != 'object' and not pd.api.types.is_categorical_dtype(X1[col_name]):
            try:
                X1.loc[:,col_name]=X1[col_name].fillna('__NaN_category__').astype(str)
            except Exception as e:
                return self
        try:
            top_n=(X1[col_name].value_counts(dropna=False).head(self.n).index.tolist())
            self.top_categories[col_name]=top_n
        except Exception as e:
            return self
        return self


    def transform(self,X):
        if len(self.cat_features) !=1:
            return X
        col_name=self.cat_features[0]
        X1=X.copy()
        if isinstance(X,np.ndarray):
            if X.ndim == 1:
                X1=pd.DataFrame({col_name:X})
            elif X.ndim == 2 and X.shape[1] == 1:
                X1=pd.DataFrame(X,columns=[col_name])
            else:
                return X
        elif isinstance(X,pd.Series):
            X1=X.to_frame(name=col_name)
        elif isinstance(X,pd.DataFrame):
            if col_name not in X.columns or len(X.columns)!= 1:
                return X
            X1=X.copy()
        else:
            return X
        if not isinstance(X1,pd.DataFrame) or col_name not in X1.columns or len(X1.columns)!=1:
            return X
        trans_X=X1.copy()
        if trans_X[col_name].dtype != 'object' and not pd.api.types.is_categorical_dtype(trans_X[col_name]):
            try:
                trans_X.loc[:,col_name]=trans_X[col_name].fillna('__NaN_category__').astype(str)
            except Exception as e:
                return X1
        top_categories1=self.top_categories.get(col_name,[])
        trans_X.loc[:,col_name]=np.where(trans_X[col_name].isin(top_categories1),trans_X[col_name],'others')
        return trans_X

"""CHECK FOR GROUP FUNCTION"""

gp2=GroupOthers(cat_features = 'trafficSource.campaign')
res=gp2.fit_transform(df[['trafficSource.campaign']])
print(res)

print(res['trafficSource.campaign'].value_counts(dropna=False))

"""EXTRACTING MEANINGFUL DATA FROM DATE FEATURE"""

class DateFeatureExtractor(BaseEstimator,TransformerMixin):
    def __init__(self,date_col='date'):
        self.date_col=date_col

    def fit(self,X,y=None):
        return self

    def transform(self,X):
        X1=X.copy()
        X1[self.date_col]=pd.to_datetime(X1[self.date_col],format = '%Y%m%d',errors='coerce')
        return pd.DataFrame({
            'year':X1[self.date_col].dt.year,
            'month':X1[self.date_col].dt.month,
            'day':X1[self.date_col].dt.day,
            'dayofweek':X1[self.date_col].dt.dayofweek,
            'quarter':X1[self.date_col].dt.quarter,
            'isweekend':(X1[self.date_col].dt.dayofweek >=5).astype(int)
        })

"""FEATURE ENGINEERING"""

df_notset = df[(df['geoNetwork.continent']=='(not set)') | (df['locationCountry']=='(not set)') ]
for_check = df_notset.groupby('browser')['purchaseValue'].agg(['count','sum'])
print(for_check)

class InitialPreprocess(BaseEstimator, TransformerMixin):

    def fit(self, X, y=None):
        self.y = y
        return self

    def transform(self, X):
        dfi = X.copy()

        dfi.replace(['', ' ', '  ', 'null', 'NULL', 'None'], 'MISSING', inplace=True)

        dfi['score'] = (dfi['pageViews'].fillna(1) * (1-dfi['totals.bounces'].fillna(0)) * dfi['totalHits'].fillna(1))
        dfi['pages_per_session'] = dfi['pageViews'] / dfi['totals.visits'].replace(0, 1)
        dfi['hits_per_page'] = dfi['totalHits'] / dfi['pageViews'].replace(0, 1)
        dfi['session_depth'] = np.log1p(dfi['pageViews'].fillna(1))
        dfi['user_experience_level'] = np.log1p(dfi['sessionNumber'].fillna(1))


        dfi['high_engagement'] = (
            (dfi['pageViews'] > 10) |
            (dfi['sessionNumber'] > 5) |
            (dfi['totalHits'] > 20)
        ).astype(int)


        dfi['is_chrome_safari'] = dfi['browser'].isin(["Chrome","Safari"]).astype(int)

        dfi['is_missing_feature'] = ((dfi['trafficSource.adContent'].isna()) | (dfi['trafficSource.adContent'] == '')).astype(int)
        dfi['is_google'] = (dfi['trafficSource.adwordsClickInfo.adNetworkType'] == 'Google').astype(int)

        dfi['trafficSource.adwordsClickInfo.page_new'] = dfi['trafficSource.adwordsClickInfo.page'].apply(self.categorize_column)

        dfi['trafficSource.campaign_new'] = dfi['trafficSource.campaign'].apply(self.process_column)
        dfi['trafficSource.keyword_new'] = dfi['trafficSource.keyword'].apply(self.process1_column)


        dfi['traffic_category'] = dfi.apply(self._categorize_traffic_source, axis=1)


        internal_domains = ['mall.googleplex.com', 'analytics.google.com']
        dfi['is_internal_traffic'] = dfi['trafficSource'].isin(internal_domains).astype(int)


        dfi['is_youtube_traffic'] = (dfi['trafficSource'] == 'youtube.com').astype(int)


        paid_mediums = ['cpc', 'cpm', 'display', 'affiliate']
        dfi['is_paid_traffic'] = dfi['trafficSource.medium'].isin(paid_mediums).astype(int)


        dfi['is_us_traffic'] = (dfi['locationCountry'] == 'United States').astype(int)


        high_value_countries = ['United States', 'United Kingdom', 'Canada']
        dfi['is_high_value_geo'] = dfi['locationCountry'].isin(high_value_countries).astype(int)


        dfi['has_city_data'] = (dfi['geoNetwork.city'].notna()).astype(int)
        dfi['has_region_data'] = (dfi['geoNetwork.region'].notna()).astype(int)






        #dfi['mobile_bounce_interaction'] = dfi['is_mobile'] * dfi['totals.bounces'].fillna(0)
        dfi['paid_engagement_interaction'] = dfi['is_paid_traffic'] * dfi['high_engagement']
        #dfi['us_mobile_interaction'] = dfi['is_us_traffic'] * dfi['is_mobile']


        dfi['bounce_rate'] = dfi['totals.bounces'].fillna(0) / dfi['totals.visits'].replace(0, 1)
        dfi['new_visitor_ratio'] = dfi['new_visits'].fillna(0) / dfi['totals.visits'].replace(0, 1)

        return dfi

    def _categorize_traffic_source(self, row):

        source = str(row.get('trafficSource', ''))
        medium = str(row.get('trafficSource.medium', ''))

        if source == 'google' and medium == 'organic':
            return 'google_organic'
        elif source == '(direct)' or medium == '(none)':
            return 'direct'
        elif source == 'youtube.com':
            return 'youtube_referral'
        elif 'google' in source and medium == 'cpc':
            return 'google_paid'
        elif medium == 'referral':
            return 'other_referral'
        elif medium in ['cpc', 'cpm']:
            return 'paid_search'
        elif medium == 'affiliate':
            return 'affiliate'
        else:
            return 'other'

    def categorize_column(self,value):
        if pd.isna(value) or value == '' or value == 1:
            return 'potential_purchase'
        else:  # values 2,3,4,5
            return 'no_purchase'

    def process_column(self,value):
        if value == '(not set)':
            return 'not_set'
        elif value == 'AW - Dynamic Search Ads Whole Site':
            return 'aw_dyna'
        elif value == 'AW - Accessories':
            return 'aw_acce'
        else:
            return 'other_values'

    def process1_column(self,value):
        if value == '(not provided)':
            return 'not_provided'
        elif value == '(Remarketing/Content targeting)':
            return 'remarket'
        elif (value == ' ') | (value == 'MISSING ') :
            return 'MISSING'
        else:
            return 'other_values'

"""LOAD TRAIN DATA FOR MODELLING"""

data= pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/train_data.csv')

X=data.drop(columns=['purchaseValue'],axis=1)
data['purchaseValue'] = data['purchaseValue'].fillna(0)
y=data['purchaseValue']

data['purchaseValue'].isnull().sum()

"""**TRAIN VALIDATION SPLIT**"""

X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=0)

"""**FEATURES AND COLUMNS TO DROP*"""

num_features=['sessionNumber','pageViews','totalHits','trafficSource.adwordsClickInfo.page','sessionStart','pages_per_session','hits_per_page','session_depth','user_experience_level','score','high_engagement']
bool_features_F=['trafficSource.isTrueDirect','device.isMobile']
bool_features_T=['trafficSource.adwordsClickInfo.isVideoAd']
bool_features_zero=['totals.bounces','new_visits','gclIdPresent','totals.visits','is_internal_traffic','is_youtube_traffic','is_paid_traffic','is_us_traffic','is_high_value_geo','has_city_data','has_region_data','paid_engagement_interaction','bounce_rate','new_visitor_ratio']
cat20_features=['geoNetwork.subContinent','geoNetwork.metro']
cat2_features =['geoCluster','geoNetwork.networkDomain','trafficSource.medium','deviceType','trafficSource.adwordsClickInfo.adNetworkType','trafficSource.adwordsClickInfo.slot','traffic_category','trafficSource.adwordsClickInfo.page_new','trafficSource.campaign_new','trafficSource.keyword_new']
cat12_features =['browser','os','userChannel','geoNetwork.continent','trafficSource.keyword','trafficSource.campaign']
cat40_features = ['locationCountry']
cat30_features = ['trafficSource']
cat50_features =['geoNetwork.region','geoNetwork.city','trafficSource.referralPath']

bool_features_one =['is_chrome_safari','is_missing_feature','is_google',]

columns_to_drop=['trafficSource.adContent','device.screenResolution','screenSize','device.mobileDeviceBranding','device.mobileInputSelector','device.mobileDeviceMarketingName','device.operatingSystemVersion','device.flashVersion','geoNetwork.networkLocation','browserMajor','device.browserSize','socialEngagementType','locationZone','device.mobileDeviceModel','device.language','device.browserVersion','device.screenColors','userId','sessionId']

"""# PIPELINES FOR IMPUTING< SCALING GROUPING AND ENCODING OF ALL FEATURES"""

num_pipe=Pipeline([
    ('imputer',SimpleImputer(strategy='most_frequent')),
    ('scaler',StandardScaler())
])

cat12_transform=[]
for col in cat12_features:
    cat12_pipe=Pipeline([
        ('astype_str',FunctionTransformer(func=lambda x:x.astype(str), validate=False)),
        ('imputer',SimpleImputer(strategy='constant',fill_value='Unknown')),
        ('group',GroupOthers(cat_features=[col],n=12)),
        ('encoder',OneHotEncoder(handle_unknown='ignore',sparse_output=False))

    ])
    cat12_transform.append((f'cat12_{col}',cat12_pipe,[col]))

cat20_transform=[]
for col in cat20_features:
    cat20_pipe=Pipeline([
        ('astype_str',FunctionTransformer(func=lambda x:x.astype(str), validate=False)),
        ('imputer',SimpleImputer(strategy='constant',fill_value='Unknown')),
        ('group',GroupOthers(cat_features=[col],n=20)),
        ('encoder',OneHotEncoder(handle_unknown='ignore',sparse_output=False))

    ])
    cat20_transform.append((f'cat20_{col}',cat20_pipe,[col]))

cat30_transform=[]
for col in cat30_features:
    cat30_pipe=Pipeline([
        ('astype_str',FunctionTransformer(func=lambda x:x.astype(str), validate=False)),
        ('imputer',SimpleImputer(strategy='constant',fill_value='Unknown')),
        ('group',GroupOthers(cat_features=[col],n=10)),
        ('encoder',OneHotEncoder(handle_unknown='ignore',sparse_output=False))

    ])
    cat30_transform.append((f'cat30_{col}',cat30_pipe,[col]))

cat40_transform=[]
for col in cat40_features:
    cat40_pipe=Pipeline([
        ('astype_str',FunctionTransformer(func=lambda x:x.astype(str), validate=False)),
        ('imputer',SimpleImputer(strategy='constant',fill_value='Unknown')),
        ('group',GroupOthers(cat_features=[col],n=20)),
        ('encoder',OneHotEncoder(handle_unknown='ignore',sparse_output=False))

    ])
    cat40_transform.append((f'cat40_{col}',cat40_pipe,[col]))

cat50_transform=[]
for col in cat50_features:
    cat50_pipe=Pipeline([
        ('astype_str',FunctionTransformer(func=lambda x:x.astype(str), validate=False)),
        ('imputer',SimpleImputer(strategy='constant',fill_value='Unknown')),
        ('group',GroupOthers(cat_features=[col],n=25)),
        ('encoder',OneHotEncoder(handle_unknown='ignore',sparse_output=False))

    ])
    cat50_transform.append((f'cat50_{col}',cat50_pipe,[col]))



cat2_transform=[]
for col in cat2_features:
    cat2_pipe=Pipeline([
        ('astype_str',FunctionTransformer(func=lambda x:x.astype(str), validate=False)),
        ('imputer',SimpleImputer(strategy='constant',fill_value='Unknown')),
        ('encoder',OneHotEncoder(handle_unknown='ignore',sparse_output=False))

    ])
    cat2_transform.append((f'cat2_{col}',cat2_pipe,[col]))


bool1_pipe=Pipeline([
    ('boolfill',boolfillint_F(bool_cols=bool_features_F)),
    ('imputer',SimpleImputer(strategy='most_frequent'))

])
bool2_pipe=Pipeline([
    ('boolfill',boolfillint_T(bool_cols=bool_features_T)),
    ('imputer',SimpleImputer(strategy='most_frequent'))

])
bool3_pipe=Pipeline([
    ('boolfill',boolfillint_zero(bool_cols=bool_features_zero)),
    ('imputer',SimpleImputer(strategy='most_frequent'))

])
bool4_pipe=Pipeline([
    ('boolfill',boolfillint_one(bool_cols=bool_features_one)),
    ('imputer',SimpleImputer(strategy='most_frequent'))

])
date_pipe = Pipeline([
    ('date_features', DateFeatureExtractor(date_col='date')),
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder',OneHotEncoder(handle_unknown='ignore',sparse_output=False))
])

"""**COLUMN TRANSFORMER**"""

col_trans=ColumnTransformer([
    ('num',num_pipe,num_features),
    #('cat2',cat2_pipe,cat2_features),
    ('bool1',bool1_pipe,bool_features_F),
    ('bool2',bool2_pipe,bool_features_T),
    ('bool3',bool3_pipe,bool_features_zero),
    ('bool4',bool4_pipe,bool_features_one),
    ('date',date_pipe,['date']),
    *cat12_transform,
    *cat20_transform,
    *cat30_transform,
    *cat40_transform,
    *cat50_transform,
    *cat2_transform,

],remainder='drop')

"""**PIPELINE FOR PREPROCESSING**"""

preprocess_pipe = Pipeline([
    ('initialpreprocess',InitialPreprocess()),
    ('drop',DropColumn(columns_to_drop)),
    ('coltransformer',col_trans),
    ('final_imputer',SimpleImputer(strategy ='most_frequent'))
   ])

mask = ~(
    ((X_train['geoNetwork.continent'] == '(not set)') |
     (X_train['locationCountry'] == '(not set)')) &
    (~X_train['browser'].isin(['Chrome', 'Safari']))
)


X_train1 = X_train[mask]
y_train1 = y_train[mask]

"""# FINAL MODEL

TRAINING OF XGBREGRESSOR MODEL WITH TRAIN DATA
"""

from sklearn.linear_model import Ridge ,Lasso
from sklearn.tree import DecisionTreeRegressor,ExtraTreeRegressor
from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,HistGradientBoostingRegressor,StackingRegressor

reg_pipe1 = Pipeline([
    ('preprocess',preprocess_pipe),
    ('regressor',ExtraTreesRegressor(
     random_state=0,
     ))
])

reg_pipe1.fit(X_train1,y_train1)

"""**VALIDATION **"""

y_pred = reg_pipe1.predict(X_val)
r2=r2_score(y_val,y_pred)
print(f"r2 score : {r2}")

"""Classification Plus regression"""

X_test1=pd.read_csv('/kaggle/input/engage-2-value-from-clicks-to-conversions/test_data.csv')

X_test1 = X_test1[X_train1.columns]
y_test_pred = reg_pipe1.predict(X_test1)

submission = pd.DataFrame({"id": range(len(y_test_pred)),
                           "purchaseValue": y_test_pred})

submission.to_csv('submission.csv',index=False)

"""```python

COMPARISION OF MODELS Linear Regression ** train score : 0.129 test score : 0.06

LGBM (n_estimators =500,learning_rate =0.1,num_leaves =31,random_state =41) train score : 0.216 test score : 0.14

RandomForest (param grid max_depth :[20,30],min_samples : [2,5], n_estimators : [50,100]) GridSearchCV (cv=3,scoring = 'r2' ,n_jobs =-1) train score : 0.280 test score : 0.18 best params ( max_depth :30 ,min_samples : 5 , n_estimators : 100

AdaBoost (learningrate [3,9], n_estimators :[10,20,30] train score : 0.06 test score : -1.22

Voting Regressor (Linear Regression and Gradient Boosting Regressor) train score : 0.407 test score :

SVM did not finish

XGBRegressor

random_state=42, learning_rate = 0.05, max_depth =15, n_estimators = 300, min_child_weight = 1, subsample = 0.8, colsample_bytree = 0.8, regressor__objective = 'reg:squarederror'

train score : 0.54 test score : 0.52

random_state=42, learning_rate = 0.03, max_depth =12, n_estimators = 500, min_child_weight = 1, subsample = 0.7, colsample_bytree = 0.7, regressor__objective = 'reg:squarederror'

train score : 0.53 test score : 0.52

COMBINATON OF CLASSIFIER AND REGRESSION

RandomForestClassifier(
                    random_state=42,
                    max_depth=10,
                    n_estimators = 100,
                    min_samples_split= 50,
                    min_samples_leaf= 50,
                    class_weight = {0: 1,1:10},
                    n_jobs=-1
    
            )  followed by

XGBRegressor(
     random_state=42,
     learning_rate  = 0.05,
     max_depth =18,
     n_estimators  = 500,
     min_child_weight  = 3,
     subsample = 0.7,
     colsample_bytree = 0.8,
     objective  = 'reg:squarederror'
    )

validation score : 0.648 Test score 0.42 VERSION 97
validation score : 0.729 Test score 0.39 VERSION 114
```



"""